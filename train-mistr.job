#!/bin/bash
#$ -M kle3@nd.edu
#$ -m abe
#$ -pe smp 8
#$ -q gpu@qa-a100-002
#$ -l gpu=0
#$ -N train-mistr
conda activate LLM
export CUDA_VISIBLE_DEVICES=2,3
# python train.py --model_name "baseline_llm" --llm_model_name "mistr-7b" --llm_frozen "False" --graph_enc "graphcl" --dataset "baseline_generation" --data "PubChem324k" --task "Description" --split "train" --num_epochs 50 --run_name "train-mistr"
python train.py --model_name "molx_llm" --llm_model_name "mistr-7b" --llm_frozen "False" --graph_enc "graphcl" --xtokens --dataset "molx_generation" --data "PubChem324k" --task "Description" --split "train" --num_epochs 50 --run_name "train-mistr" --checkpoint_path "output/training/molx_generation/PubChem324k/molx_llm_mistr-7b_llm_frozenTrue_graphcl_chemberta_5epochs_lr1e-05_pretrain_best_pretrain-mistr.pth"